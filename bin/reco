#!/usr/bin/env ruby

java_import "org.deeplearning4j.models.embeddings.loader.WordVectorSerializer"
java_import "org.deeplearning4j.models.word2vec.Word2Vec"

NN_ROOT = File.join(File.dirname(__FILE__), '..')

class Repository
  def initialize
  end

  def items
    items = []

    IO.foreach(File.join(::NN_ROOT, "data/reco/tweets.txt")) do |line|
      doc_id, doc_title, doc_url, doc_domain, screen_name = line.chomp.split("\t")

      items << ({
        id: doc_id.to_i,
        title: doc_title,
        url: doc_url,
        screen_name: screen_name,
        domain: doc_domain
      })
    end

    items
  end
end

class Documents
  def initialize(file)
    @documents = {}
    @users = {}

    IO.foreach(file) do |line|
      id, url, *users = line.split(' ')
      @documents[url] = users
      users.each do |u|
        @users[u] = true
      end
    end
  end

  def users(url)
    @documents[url]
  end

  def total_users_count
    @users.size
  end
end

documents_file = File.join(NN_ROOT, "data", "reco", "docs.txt")
documents      = Documents.new(documents_file)

class User2Vec
  def initialize(file)
    @model = WordVectorSerializer.loadTxtVectors(java.io.File.new(file))
  end

  def similarity(user1, user2)
    s = 0.0
    begin
      if (@model.indexOf(user1) > 0) && (@model.indexOf(user2) > 0)
        s = @model.similarity(user1, user2)
      end
    rescue ::Exception => e
    end
    s
  end

  def neighbours(u, k)
    @model.wordsNearest(u, k)
  end
end

user_scores = {}

userstats_file = File.join(::NN_ROOT, "data", "reco", "userstats.txt")

IO.foreach(userstats_file) do |l|
  screen_name, docs_per_day, cc,  ft, days = l.chomp.split('|')
  #user_scores[screen_name] = [25 / docs_per_day.to_f, 1.0].min
  #user_scores[screen_name] = [2.5 / docs_per_day.to_f, 1.0].min
  user_scores[screen_name] = 1.0
end

items = Repository.new.items

docs      = {}
doc_users = {}
doc_scores = {}

items.each do |item|
  docs[item[:url]] = item
  unless doc_users.key?(item[:url])
    doc_users[item[:url]] = []
    doc_scores[item[:url]] = 0.0
  end

  doc_users[item[:url]] << item[:screen_name]
  doc_scores[item[:url]] += user_scores[item[:screen_name]].to_f
end

domain_stats = {}
doc_scores.sort_by { |k,v| -v }.each do |(doc_url,doc_score)|
  url    = docs[doc_url][:url]
  domain = docs[doc_url][:domain]

  domain_stats[domain] = {urls: {}, count: 0, total_score: 0.0} unless domain_stats[domain]
  unless domain_stats[domain][:urls].key?(url)
    domain_stats[domain][:urls][url] = domain_stats[domain][:urls].size
    domain_stats[domain][:count] += 1
  end

  domain_stats[domain][:total_score] += doc_score
end

best_score = doc_scores.sort_by { |k,v| -v }.first[1]

doc_scores.sort_by { |k,v| -v }.each do |(doc_url,doc_score)|
  domain               = docs[doc_url][:domain]
  percent_score_domain = 0.25
  if domain_stats[domain][:total_score] > 0
    percent_score_domain = doc_score.to_f / domain_stats[domain][:total_score]
  end

  doc_scores[doc_url] = (doc_score**0.75) + (percent_score_domain*doc_score)
end

user2vec_file = File.join(NN_ROOT, "data", "reco", "user2vec.txt")
user2vec      = User2Vec.new(user2vec_file)

click_stream = [
  "http://digiday.com/publishers/two-months-four-things-npr-learned-using-facebook-live",
  "http://meta-media.fr/2016/06/30/la-nouvelle-economie-de-la-baladodiffusion.html",
  "http://www.lemonde.fr/politique/article/2016/06/29/sarkozy-promet-de-revenir-sur-le-non-cumul-des-mandats-et-s-attire-des-critiques-a-droite_4960797_823448.html",
  "http://www.theguardian.com/technology/2016/jul/05/google-deepmind-nhs-machine-learning-blindness",
  "http://www.slate.fr/story/120889/thomas-dixon-twitter-amnesie-memoire-internet",
  "http://www.liberation.fr/planete/2016/07/01/greenpeace-repond-a-l-appel-des-prix-nobel-pro-ogm_1463183",
  "http://techcrunch.com/2016/07/09/pokemon-go-ar-hype-check",
  "http://www.nytimes.com/2016/07/11/business/media/as-online-video-surges-publishers-turn-to-automation.html",
  "http://www.atelier.net/trends/articles/enquete-vers-une-collaboration-entre-humains-machines-monde-travail_442069",
  "http://www.marianne.net/les-casseurs-sont-instrumentalises-pouvoir-accuse-cgt-police-100243787.html",
  "http://singularityhub.com/2016/06/17/long-promised-artificial-intelligence-is-looming-and-its-going-to-be-amazing",
  "http://uxmag.com/articles/ux-trends-to-keep-in-mind-for-designing-in-2016",
  "http://leplus.nouvelobs.com/contribution/1527186-les-etudiants-de-grandes-ecoles-s-approprient-le-doctorat-juste-pour-leur-carte-de-visite.html",
  "http://www.lequipe.fr/explore-video/islande-tout-sauf-un-miracle"
]

click_stream.each do |url|
  users = documents.users(url)
  total_users_count = documents.total_users_count

  reco_scores = {}

  doc_users.select { |k,v| v.size > 1 }.each do |doc_url,_users|
    doc_score  = 0.0
    users_size = 10
    users.each do |u1|
      _users.uniq.map { |u2| user2vec.similarity(u1, u2) }.first(users_size).each do |sim|
        doc_score += (sim**5)
      end
    end
    doc_score = doc_score / users.size
    reco_scores[doc_url] = doc_score
  end

  p "----------U2VEC---------"
  p url
  reco_scores.sort_by { |k,v| -v }.first(7).each do |(k,v)|
    p [v, doc_users[k].size, v / doc_users[k].size, docs[k][:title], k]
  end
end

k = 15
neighbours = user2vec.neighbours('toto_flint', k)

reco_scores = {}

doc_users.select { |k,v| v.size > 1 }.each do |doc_url,_users|
  doc_score  = 0.0
  users_size = 10
  neighbours.each do |u1|
    _users.uniq.map { |u2| user2vec.similarity(u1, u2) }.first(users_size).each do |sim|
      doc_score += (sim**5)
    end
  end
  doc_score = doc_score / neighbours.size
  reco_scores[doc_url] = doc_score
end

p "----------KNN toto_flint - U2VEC---------"
reco_scores.sort_by { |k,v| -v }.first(10).each do |(k,v)|
  p [v, doc_users[k].size, v / doc_users[k].size, docs[k][:title], k]
end
